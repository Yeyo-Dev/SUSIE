# ü§ñ An√°lisis de Integraci√≥n ‚Äî 4 Modelos de IA en SUSIE

## Resumen

Los 4 modelos de IA est√°n **bien implementados individualmente** y siguen un patr√≥n consistente. Sin embargo, **NO se pueden integrar tal cual** porque hay **piezas clave faltantes** que conectan los modelos con el backend y entre s√≠.

---

## Estado Actual de Cada Modelo

### ‚úÖ Lo que S√ç est√° listo

| Modelo | Archivo | Patr√≥n | Output |
|--------|---------|--------|--------|
| **YOLO Vision** | [vision_yolo/main.py](file:///c:/Users/Yeyo_/OneDrive/Documentos/GitHub/SUSIE/ai_models/vision_yolo/main.py) | RabbitMQ Worker ‚Üí Redis | Evento universal con `severity`, `event_type`, `details` |
| **Whisper Audio** | [audio_whisper/main.py](file:///c:/Users/Yeyo_/OneDrive/Documentos/GitHub/SUSIE/ai_models/audio_whisper/main.py) | RabbitMQ Worker ‚Üí Redis | Evento universal (id√©ntico formato) |
| **Gaze MediaPipe** | [gaze_mediapipe/main.py](file:///c:/Users/Yeyo_/OneDrive/Documentos/GitHub/SUSIE/ai_models/gaze_mediapipe/main.py) | RabbitMQ Worker ‚Üí Redis | Evento universal (id√©ntico formato) |
| **DeepFace Biom√©trico** | [biometric_deepface/main.py](file:///c:/Users/Yeyo_/OneDrive/Documentos/GitHub/SUSIE/ai_models/biometric_deepface/main.py) | FastAPI HTTP s√≠ncrono | JSON `{ auth, distance, message }` |

> [!TIP]
> El **formato universal de eventos** es consistente para 3 de los 4 modelos ‚Äî esto es excelente porque facilita mucho la integraci√≥n.

### Formato Universal que comparten YOLO, Whisper y Gaze:
```json
{
  "timestamp": "2026-...",
  "userId": "student_id",
  "sessionId": "session_id",
  "source": "yolo_vision | audio_nlp | gaze_tracker",
  "severity": "INFO | WARNING | CRITICAL",
  "event_type": "...",
  "details": { ... }
}
```

---

## üö® Lo que FALTA para la Integraci√≥n Completa

### 1. Docker Compose no incluye los AI Workers

**Problema:** El [docker-compose.yml](file:///c:/Users/Yeyo_/OneDrive/Documentos/GitHub/SUSIE/docker-compose.yml) actual solo define RabbitMQ, PostgreSQL y Redis. **Ninguno de los 4 servicios de IA est√° declarado**.

**Qu√© falta:**
```yaml
# Estos 4 servicios faltan totalmente en docker-compose.yml
services:
  yolo-worker: ...
  whisper-worker: ...
  gaze-worker: ...
  deepface-api: ...
```

---

### 2. Backend (Fastify) no publica mensajes a RabbitMQ

**Problema:** Los workers de IA (YOLO, Whisper, Gaze) escuchan en colas de RabbitMQ (`q_snapshots`, `q_audios`, `gaze_tasks_queue`), pero el backend Fastify **no tiene un producer que publique mensajes** a esas colas. 

El [contrato de integraci√≥n](file:///c:/Users/Yeyo_/OneDrive/Documentos/GitHub/SUSIE/CONTRATO_INTEGRACION_BACKEND.md) define los endpoints HTTP que reciben evidencia del frontend, pero la l√≥gica para:
1. Subir el archivo a Azure Blob Storage
2. Publicar un mensaje en RabbitMQ con la URL del blob

...no est√° implementada.

**Colas esperadas por los workers:**

| Worker | Cola RabbitMQ | Payload esperado |
|--------|--------------|------------------|
| YOLO | `q_snapshots` | `{ student_id, session_id, image_url }` |
| Whisper | `q_audios` *(tiene espacio extra al final!)* | `{ student_id, session_id, audio_url, chunk_index }` |
| Gaze | `gaze_tasks_queue` | `{ student_id, session_id, gaze_buffer: [(x,y)...] }` |

---

### 3. Motor de Inferencia est√° vac√≠o

**Problema:** El archivo [inference_engine/main.py](file:///c:/Users/Yeyo_/OneDrive/Documentos/GitHub/SUSIE/inference_engine/main.py) es un **esqueleto** que solo tiene un `# AI Logic goes here`. No hay l√≥gica de correlaci√≥n/fusi√≥n de se√±ales.

Seg√∫n la arquitectura, el motor deber√≠a:
- Leer eventos de Redis de los 3 workers
- Usar una **Red Bayesiana Na√Øve** (pgmpy) para correlacionar se√±ales
- Calcular un **risk score** final por sesi√≥n
- Persistir resultados en PostgreSQL

---

### 4. No hay flujo Redis ‚Üí PostgreSQL

**Problema:** Los 3 workers as√≠ncronos (YOLO, Whisper, Gaze) escriben eventos al Redis en la key `proctoring:session_{sid}:user_{uid}`, pero **nada lee esos eventos de Redis para persistirlos** en PostgreSQL ni para generar el reporte final.

---

### 5. DeepFace no est√° conectado al flujo del Backend

**Problema:** DeepFace opera como API HTTP standalone (FastAPI), pero el backend Fastify **no tiene proxy/integraci√≥n** para llamar `/api/register` o `/api/validate` de DeepFace.

Adem√°s:
- DeepFace usa [database.json](file:///c:/Users/Yeyo_/OneDrive/Documentos/GitHub/SUSIE/ai_models/biometric_deepface/database.json) como persistencia local, no PostgreSQL
- No tiene autenticaci√≥n (cualquiera puede llamar los endpoints)
- Se necesitan endpoints en Fastify que deleguen las llamadas biom√©tricas

---

### 6. Nombres de colas con inconsistencias

**Problema menor:** En [audio_whisper/main.py](file:///c:/Users/Yeyo_/OneDrive/Documentos/GitHub/SUSIE/ai_models/audio_whisper/main.py) l√≠nea 141:
```python
QUEUE_NAME = 'q_audios '  # <--- Espacio extra al final!
```
Esto causar√° un **bug silencioso** donde el producer publica a `q_audios` pero el worker escucha en `q_audios ` (con espacio).

---

### 7. Sin retry/reconnect robusto en los workers

**Problema:** Los 3 workers de RabbitMQ solo hacen un intento de conexi√≥n. Si falla, imprimen un mensaje y **terminan sin reintentar**. No hay loop de retry con backoff exponencial.

```python
# C√≥digo actual ‚Äî sale sin reintentar
except pika.exceptions.AMQPConnectionError:
    print(f"[!] No se pudo conectar...")
```

---

### 8. Azure Blob Storage no est√° configurado

Ning√∫n worker tiene configuraci√≥n de Azure Blob Storage. Los workers asumen que reciben URLs directas para descargar, lo cual es correcto, pero el backend Fastify (que debe subir los blobs y generar las URLs) no tiene esa l√≥gica implementada.

---

## Hoja de Ruta de Integraci√≥n

Ordenado por prioridad y dependencia:

| # | Tarea | Componente | Impacto |
|---|-------|-----------|---------|
| 1 | Implementar **RabbitMQ Producer** en backend Fastify | `backend/` | üî¥ Bloqueante |
| 2 | Implementar **Azure Blob Upload** en backend | `backend/` | üî¥ Bloqueante |
| 3 | Agregar los **4 servicios AI al docker-compose** | [docker-compose.yml](file:///c:/Users/Yeyo_/OneDrive/Documentos/GitHub/SUSIE/docker-compose.yml) | üî¥ Bloqueante |
| 4 | Implementar **Motor de Inferencia** (leer Redis ‚Üí Naive Bayes ‚Üí PostgreSQL) | `inference_engine/` | üü° Necesario para reportes |
| 5 | Integrar **DeepFace** como servicio interno llamable por Fastify | `backend/ + biometric_deepface/` | üü° Necesario para biometr√≠a |
| 6 | Corregir **bug del espacio** en nombre de cola `q_audios` | `audio_whisper/main.py:141` | üü¢ Fix r√°pido |
| 7 | Agregar **retry con backoff** a los 3 workers | `ai_models/*/main.py` | üü¢ Mejora de resiliencia |
| 8 | Implementar **endpoints de reportes** (`/api/reports/...`) | `backend/` | üü° Necesario para Fase 8 |

> [!IMPORTANT]
> Las tareas **1, 2 y 3** son absolutamente bloqueantes. Sin ellas, los modelos de IA no reciben datos y no se ejecutan. El frontend env√≠a evidencia HTTP al backend, pero el backend no la enruta a los workers.

---

## Conclusi√≥n

Los 4 modelos est√°n **bien construidos y listos para funcionar**, pero les falta la **"fontaner√≠a" que los conecta**:

```mermaid
flowchart LR
    FE["Frontend"] -->|"HTTP POST"| BE["Backend Fastify"]
    BE -->|"‚ùå No implementado"| AZ["Azure Blob"]
    BE -->|"‚ùå No implementado"| RMQ["RabbitMQ"]
    RMQ -->|"‚úÖ Listo"| YOLO["Worker YOLO"]
    RMQ -->|"‚úÖ Listo"| WSP["Worker Whisper"]
    RMQ -->|"‚úÖ Listo"| GAZE["Worker Gaze"]
    YOLO & WSP & GAZE -->|"‚úÖ Escriben"| REDIS["Redis"]
    REDIS -->|"‚ùå No implementado"| INF["Motor de Inferencia"]
    INF -->|"‚ùå No implementado"| PG["PostgreSQL"]
    BE -->|"‚ùå No implementado"| DF["DeepFace API"]
    
    style AZ fill:#ff6b6b,color:#fff
    style RMQ fill:#ff6b6b,color:#fff
    style INF fill:#ff6b6b,color:#fff
    style DF fill:#ff6b6b,color:#fff
    style YOLO fill:#51cf66,color:#fff
    style WSP fill:#51cf66,color:#fff
    style GAZE fill:#51cf66,color:#fff
    style REDIS fill:#51cf66,color:#fff
```

En resumen: **los workers est√°n listos pero aislados**. Necesitan que el backend les alimente datos (producer ‚Üí RabbitMQ) y que alguien consuma sus resultados (motor de inferencia ‚Üí PostgreSQL ‚Üí reportes).
